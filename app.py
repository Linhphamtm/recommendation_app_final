# FINAL APP - FULL CLEAN & STABLE VERSION
# Ng∆∞·ªùi th·ª±c hi·ªán: Ph·∫°m Th·ªã Mai Linh
# Ng√†y b√°o c√°o: 13/04/2025

import streamlit as st
import pandas as pd
import numpy as np
import ast
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split

# =============================
# CONFIG APP
# =============================
st.set_page_config(page_title='H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m', layout='wide', initial_sidebar_state='expanded')

# =============================
# SIDEBAR
# =============================
st.sidebar.title('üìÇ ƒêi·ªÅu h∆∞·ªõng')
page = st.sidebar.radio('Ch·ªçn m·ª•c', ['Insight', 'App'])

st.sidebar.markdown("""
### üßë‚Äçüíª Ng∆∞·ªùi th·ª±c hi·ªán
**Ph·∫°m Th·ªã Mai Linh**

### üìÖ Ng√†y b√°o c√°o
**13/04/2025**
""")

# =============================
# LOAD DATA & TRAIN MODELS
# =============================

# Product-based model
# Load data
df_product = pd.read_csv('cleaned_products.csv')
df_product['final_cleaned_tokens'] = df_product['final_cleaned_tokens'].apply(ast.literal_eval)

# Load stopwords
with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:
    stop_words = set(f.read().splitlines())

# L√†m s·∫°ch tokens
content_clean = [
    [
        re.sub(r'[0-9]+', '', token.lower())
        for token in tokens
        if token.lower() not in stop_words and token not in ['', ' ', ',', '.', '...', '-', ':', ';', '?', '%', '(', ')', '+', '/', "'", '&']
    ]
    for tokens in df_product['final_cleaned_tokens']
]

# Vector h√≥a TF-IDF
documents = [' '.join(tokens) for tokens in content_clean]
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# User-based model
# Load data g·ªëc
df_user = pd.read_csv('Products_ThoiTrangNam_rating_raw.csv', sep='\t')
df_user = df_user.drop_duplicates().drop_duplicates(subset=['user_id', 'product_id'], keep='first')
df_sample = df_user.sample(n=10000, random_state=42).reset_index(drop=True)

reader = Reader(rating_scale=(df_sample['rating'].min(), df_sample['rating'].max()))
data = Dataset.load_from_df(df_sample[['user_id', 'product_id', 'rating']], reader)
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

algo = SVD()
algo.fit(trainset)

# Ch·ªâ l·∫•y user_id c√≥ product_id tr√πng v·ªõi df_product
valid_user_ids = df_sample[df_sample['product_id'].isin(df_product['product_id'])]['user_id'].unique().tolist()

# =============================
# INSIGHT PAGE
# =============================
if page == 'Insight':
    st.title('üìä Project Insight')

    st.header('üéØ M·ª•c ti√™u project')
    st.markdown("""
    X√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m th·ªùi trang nam tr√™n Shopee, nh·∫±m h·ªó tr·ª£ ng∆∞·ªùi ti√™u d√πng d·ªÖ d√†ng t√¨m ki·∫øm s·∫£n ph·∫©m ph√π h·ª£p d·ª±a tr√™n:

    - **G·ª£i √Ω theo n·ªôi dung s·∫£n ph·∫©m:** D·ª±a tr√™n m√¥ t·∫£ chi ti·∫øt c·ªßa s·∫£n ph·∫©m.
    - **G·ª£i √Ω theo ng∆∞·ªùi d√πng:** D·ª±a tr√™n l·ªãch s·ª≠ ƒë√°nh gi√° v√† t∆∞∆°ng t√°c c·ªßa ng∆∞·ªùi d√πng.
    """)

    st.header('üìä Kh√°m ph√° d·ªØ li·ªáu (EDA)')
    st.subheader('Wordcloud m√¥ t·∫£ s·∫£n ph·∫©m')
    text = ' '.join([' '.join(tokens) for tokens in content_clean])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)

    st.subheader('Ph√¢n ph·ªëi rating s·∫£n ph·∫©m')
    st.bar_chart(df_product['rating'].value_counts().sort_index())

    st.subheader('Ph√¢n ph·ªëi rating t·ª´ ng∆∞·ªùi d√πng')
    st.bar_chart(df_sample['rating'].value_counts().sort_index())

    st.header('üßπ C√°c b∆∞·ªõc l√†m s·∫°ch d·ªØ li·ªáu')
    st.markdown("""
    **B∆∞·ªõc 1:** Chu·∫©n h√≥a vƒÉn b·∫£n m√¥ t·∫£ s·∫£n ph·∫©m.

    **B∆∞·ªõc 2:** Lo·∫°i b·ªè nhi·ªÖu v√† pattern kh√¥ng mong mu·ªën.

    **B∆∞·ªõc 3:** Tokenization v√† lo·∫°i b·ªè stopword.

    **B∆∞·ªõc 4:** K·∫øt qu·∫£: VƒÉn b·∫£n s·∫°ch trong `final_cleaned_tokens`.
    """)

    st.header('üß© Thu·∫≠t to√°n s·ª≠ d·ª•ng')
    st.markdown("""
    - **Cosine Similarity:** ƒêo m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c s·∫£n ph·∫©m.
    - **Surprise SVD:** D·ª± ƒëo√°n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi t·ª´ng ng∆∞·ªùi d√πng.
    """)

    st.header('üìà ƒê√°nh gi√° thu·∫≠t to√°n')
    st.markdown("""
    **Cosine Similarity:**
    - ∆Øu ƒëi·ªÉm: D·ªÖ tri·ªÉn khai, tr·ª±c quan.
    - H·∫°n ch·∫ø: Kh√¥ng c√° nh√¢n h√≥a.

    **Surprise SVD:**
    - ∆Øu ƒëi·ªÉm: C√° nh√¢n h√≥a theo l·ªãch s·ª≠ ng∆∞·ªùi d√πng.
    - H·∫°n ch·∫ø: C·∫ßn ƒë·ªß d·ªØ li·ªáu ng∆∞·ªùi d√πng ƒë·ªÉ hu·∫•n luy·ªán.
    """)

# =============================
# APP PAGE (RECOMMENDATION)
# =============================
elif page == 'App':
    st.title('ü§ñ Recommendation App')

    tab1, tab2 = st.tabs(['üõçÔ∏è G·ª£i √Ω theo s·∫£n ph·∫©m', 'üë• G·ª£i √Ω theo ng∆∞·ªùi d√πng'])

    # Product-based tab
    with tab1:
        st.header('G·ª£i √Ω theo s·∫£n ph·∫©m (Content-based)')
        user_input = st.text_input('Nh·∫≠p m√¥ t·∫£ s·∫£n ph·∫©m b·∫°n mu·ªën t√¨m g·ª£i √Ω:')
        top_k = st.slider('S·ªë l∆∞·ª£ng s·∫£n ph·∫©m g·ª£i √Ω:', 1, 20, 5)

        if st.button('üìä Hi·ªÉn th·ªã g·ª£i √Ω s·∫£n ph·∫©m'):
            if not user_input.strip():
                st.warning('‚ö†Ô∏è Vui l√≤ng nh·∫≠p m√¥ t·∫£ s·∫£n ph·∫©m.')
            else:
                user_input_vector = vectorizer.transform([user_input])
                sim_scores = list(enumerate(cosine_similarity(user_input_vector, tfidf_matrix)[0]))
                sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_k]

                recommendations = []
                for idx, score in sim_scores:
                    row = df_product.iloc[idx]
                    recommendations.append({
                        '·∫¢nh': row['image'] if pd.notna(row['image']) else '',
                        'T√™n s·∫£n ph·∫©m': row['product_name'],
                        'Gi√°': f"{row['price']:.0f} VND" if pd.notna(row['price']) else 'N/A',
                        'ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng': f"{score:.2f}",
                        'Link': f"[Xem s·∫£n ph·∫©m]({row['link']})" if pd.notna(row['link']) else "N/A"
                    })

                st.markdown('### üéØ K·∫øt qu·∫£ g·ª£i √Ω:')
                for rec in recommendations:
                    cols = st.columns([1, 3])
                    if rec['·∫¢nh']:
                        cols[0].image(rec['·∫¢nh'], width=120)
                    cols[1].markdown(f"**{rec['T√™n s·∫£n ph·∫©m']}**")
                    cols[1].markdown(f"üí∞ {rec['Gi√°']} | ‚≠êÔ∏è ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {rec['ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng']}")
                    cols[1].markdown(f"üîó {rec['Link']}")
                    cols[1].markdown('---')

                results_df = pd.DataFrame(recommendations)
                csv = results_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button('üì• T·∫£i k·∫øt qu·∫£ v·ªÅ CSV', data=csv, file_name='recommendations_product.csv', mime='text/csv')

    # User-based tab
    with tab2:
        st.header('G·ª£i √Ω theo ng∆∞·ªùi d√πng (Collaborative filtering)')
        selected_user = st.selectbox('Ch·ªçn User b·∫°n mu·ªën t√¨m g·ª£i √Ω:', valid_user_ids)
        top_k_user = st.slider('S·ªë l∆∞·ª£ng s·∫£n ph·∫©m g·ª£i √Ω:', 1, 20, 5, key='user_slider')

        if st.button('üìä Hi·ªÉn th·ªã g·ª£i √Ω ng∆∞·ªùi d√πng'):
            user_ratings = df_sample[df_sample['user_id'] == selected_user]
            if user_ratings.empty:
                st.warning('‚ö†Ô∏è Ng∆∞·ªùi d√πng n√†y kh√¥ng c√≥ ƒë√°nh gi√° trong d·ªØ li·ªáu!')
            else:
                all_product_ids = df_product['product_id'].unique()
                predictions = [(product_id, algo.predict(selected_user, product_id).est) for product_id in all_product_ids]
                predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_k_user]

                recommendations = []
                for pid, est in predictions:
                    row = df_product[df_product['product_id'] == pid].iloc[0]
                    recommendations.append({
                        '·∫¢nh': row['image'] if pd.notna(row['image']) else '',
                        'T√™n s·∫£n ph·∫©m': row['product_name'],
                        'Gi√°': f"{row['price']:.0f} VND" if pd.notna(row['price']) else 'N/A',
                        'Rating d·ª± ƒëo√°n': f"{est:.2f}",
                        'Link': f"[Xem s·∫£n ph·∫©m]({row['link']})" if pd.notna(row['link']) else "N/A"
                    })

                st.markdown('### üéØ K·∫øt qu·∫£ g·ª£i √Ω:')
                for rec in recommendations:
                    cols = st.columns([1, 3])
                    if rec['·∫¢nh']:
                        cols[0].image(rec['·∫¢nh'], width=120)
                    cols[1].markdown(f"**{rec['T√™n s·∫£n ph·∫©m']}**")
                    cols[1].markdown(f"üí∞ {rec['Gi√°']} | ‚≠êÔ∏è Rating d·ª± ƒëo√°n: {rec['Rating d·ª± ƒëo√°n']}")
                    cols[1].markdown(f"üîó {rec['Link']}")
                    cols[1].markdown('---')

                results_df_user = pd.DataFrame(recommendations)
                csv = results_df_user.to_csv(index=False).encode('utf-8-sig')
                st.download_button('üì• T·∫£i k·∫øt qu·∫£ v·ªÅ CSV', data=csv, file_name='recommendations_user.csv', mime='text/csv')